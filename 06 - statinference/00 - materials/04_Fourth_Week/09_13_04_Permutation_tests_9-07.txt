We're going to end the class on
something that is an incredibly use fur,
useful tool for data scientists,
so called permutation tests, and
these are used for group comparisons, so
let's start with a motivating example of
group comparisons we may, where it may
not be immediately obvious what to do.
So in this example researchers
took batches of insects and
applied different pesticides to them,
labeled as sprays here.
And they counted the number of
dead insects in each batch.
So for each spray,
they had a collection of counts that were
presented the efficacy of that pesticide.
Let's consider looking at insect
spray B versus insect spray C.
Consider the null hypothesis
that the distribution of
the observations from each group is
the same in other words which label,
particular count received was irrelevant.
So operationally think about a data
frame with the counts down one column,
and the spray group labels
down another column.
We would calculate a statistic such
as the difference in the average
number of insects killed for group b and
subtract that off from group c,
that would be our observed test statistic.
Now consider permuting the group labels,
just taking that vector of labels and
permuting it for
example with just the command sample in r,
which if you give a vector with
no arguments the command sample will
just randomly permutate the entries.
Then you could recalculate
the statistic for
each permutation,
you could use any statistic you like.
For example, it could be the mean
difference in counts between the two
groups, it could be the geometric means or
even a T statistic,
you can use a T statistic and
not compare it with a T null distribution,
you could compare it to a permutation
based null distribution.
To calculate a P-value, you simply want to
calculate the percentage of simulations
where the simulated statistic was more
extreme, in the favor of the alternative,
than the observed.
So more extreme, in the difference
of mean settings, would be
having a greater difference in the means,
towards the direction of the alternative.
This will yeed,
yield a permutation based P-value.
Let's go through an example and
I think it'll be mu.
I'm not going to dwell too
much on this slide, but
I just want to say that permutation
test are very powerful,
and so much so that they keep being
reinvented in different setting.
For example, the rank sum test,
a famous test and
statistic, is a permutation test where
you've replaced your data in some fashion
by ranks,
rather than the original raw values.
Fisher's exact test,
which you might have heard of,
is exactly a permutation based test,
where simply,
our data type is binary, and there's
a specific test statistic you're using.
If you use the raw the data then you're
just doing an ordinary permutation test.
I would also add that there's a separate
entity called randomization test,
when you explicitly had randomization
your group labels were randomized.
For example in our insect spray test,
batches might have had sprays
randomized to them, so it might be
considered a ram- randomization test.
Oper- Operationally, your randomization
test is going to to performed in the same
way as a permutation test, however you
might have stronger conclusions from your
randomization test and the way in
which you interpret your randomization
test might be a little bit different.
I would also add you can use permutation
strategies in regression, so
you perm, just simply permuate a regressor
and this is a different way to get a null
distribution than using the normal
distributions that we'll cover
in our regression class as,
as the next part of the specialization.
In permutation tests work very well
with multi variant settings, because you
can calculate sort of maximum statistics
that control family wide error rates.
We're not going to go over
too much of any of this,
we simply want you to be able to go
through a simple permutation test,
get the ideas, so later on you can
build on top of this foundation.
Let's go through an example where we
just illustrate doing permutation tests.
First I'm going to subset the data just so
I have InsectSprays B and C.
My y's going to be my outcome,
my count of dead insects.
My group is just going to be group labels,
or
spray in this case pesticide in this case.
So y element one received is
the count of dead insects for
InsectSpray element one
of the vector group, and
the same thing for element two of y and
element two of group in other words,
my y and
my group vectors are perfectly lined up.
So my test statistic is just
the average difference,
the mean average number of
dead insects from group B
minus the average number of dead insects
in group C averaged over batches.
My actual observed statistic is
just my testStat applied to my sp,
my outcome and group here,
the two are lined up correctly.
Now, I'm going to break that
line-up by permuting, so
now when I apply my test statistic
I'm going to give it my y but
my group label is permuted so I'm doing
sample group which permute my group label
and i,m going to do that 10,000
to get 10,000 test statistics,
where I've broken any association between
y and group, by permuting my group labels.
This is under the null hypothesis that my
group label is unrelated to my outcome.
This means that the 13.25
is the difference
in the average count of dead insects for
B, minus dead insects for C, so
in excess of 13 dead insects on
average killed per batch for
B versus C.
Let's now calculate the percentage
of our permuted test statistics that
are larger or more extreme in favor of the
alternative in our observed statistic and
it turns out in this data
set that we get zero.
So across 10,000 permutations,
we couldn't find a reconfiguration of
the group labels that leaded, that
lead to a more extreme value of the test
statistic than our observed statistic.
Or, more formally the P-value
is very small close to zero and
so we reject the null hypothesis for
any reasonable level of alpha.
I would note the P-value is not exactly
zero because we can think of at
least one permutation, that is as big
as our observed statistic, namely,
the permutation that gives
us back our original data.
This is kind of a minor point, because
we're going to do thousands and thousands
of simulations and whether or not we add
that one in or not is kind of irrelevant
when we compare our p val, p value to
any normal critical value like 5%.
Here I'm showing the null distribution
generated by our permutations.
Basically this is serving the same
role that our t distribution or
standard z distribution.
Does in hypothesis testing where we're
willing to assume that either the data is
normal, or we're appealing to
the central limit theorem.
We'll see that our null distribution
in this case is centered at zero, and
goes from minus 10 to plus 10, so
the fact that we see a minus 10 means that
when we permuted group labels we saw
test statistics as low as minus 10.
What I put here as a vertical
line is the observed statistic,
and you'll see that it's way far out
in the tail of our null distribution.
Suggesting that likely
the null distribution
is not the correct true distribution in
that B really is very different from C,
I think we can just see
that from the histogram but
now we can quantify it with a P-value, and
we can perform a formal hypothesis test.
Whether in bootstrapping where
you're looking at the s-,
sampling distribution of a statistic.
Or in permutation testing,
we are looking at a formal inference
based on an idea of exchangability
of group labels In either case,
I think it's very important
to do these histograms or
density estimates to see your
resampled distributions.

