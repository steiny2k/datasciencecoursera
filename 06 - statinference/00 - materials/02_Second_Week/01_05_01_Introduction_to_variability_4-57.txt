Hi, my name is Brian Caffo, and
this is a lecture on Variability as part
of the Statistical Inference Coursera
class, which is part of
the Data Science specialization from
the Johns Hopkins Bloomberg School of
Public Health Department of Biostatistics.
So in the last lecture,
we talked about the population mean
which talks about where
the distribution is centered at.
So if you were to think about a bell
curve, that probability density function
will move to the left or
the right as the mean changes.
Another useful property is how fat or
how thin, or how spread out or
how concentrated the density is around
the mean and that's the variance.
So if X is a random variable that has mean
mu, the variance is exactly the expected
square of distance the random variable is
from the mean and I give the formula here.
There's a nice shortcut one that
is the expected value of X squared
minus the expected value
of X quantity squared.
So densities with higher variance
are more spread out than densities with
lower variances and the square root of the
variance is called the Standard Deviation.
So the variance is expressed in the unit
squared, whereas the standard deviation is
expressed in the same units as X,
which is quite useful.
In this class, we won't spend a lot
of time calculating expected values,
either means or
variances of populations by hand, but
I want to go through one such calculation.
Recall from the last lecture,
the expected value of X is 3.5 when
X is the result of the toss of a die.
The expected value of X squared, I really
haven't given you a formula how to
do that, but really think of it as
the expected value of the random variable
that you get by rolling a die,
then squaring the result.
And you can do that by simply
taking the number, for
example 1 squared, 2 squared,
3 squared, 4 squared, 5 squared and
multiplying by their associated
probabilities and you get 15.17.
So if I were to subtract
15.17 minus 3.5 squared,
I get 2.92 which is the variance of this,
of a die roll.
Since that was so much fun,
let's do another example.
So imagine the toss of a coin with
the probability of heads of p.
We already covered that the expected
value of a coin toss is p
from the last lecture and then let's think
about the expected value of X squared.
When this case, 0 squared is 0 and
1 squared is 1, so
the expected value of X squared is exactly
the expected value of X, which is p.
Now if we were to plug into our formula,
we then get p minus p squared,
which works out to be p times 1 minus p.
So in other words, the variance,
the population variance associated with
the distribution given
by the flip of a coin,
a biased coin is exactly
p times 1 minus p.
This is a very famous formula and
I'd recommend that you
just commit it to memory.
Here, I'm giving you some
examples of densities,
population densities as
the variance changes.
The salmon colored density is
a standard normal which has variance 1.
As I go up, you see the variance
increases, it squashes the density down,
and it pushes more of
the mass out into the tails.
So there's more likely that a person say,
is beyond 5, if for
example they are from the normal
distribution with a variance of 4,
than if they were from a normal
distribution with a variance of 3.
So just like the population mean and
the sample mean were directly analogous,
the population variance and the sample
variance are dir, directly analogous.
So for example, the population mean was
the center of mass of the population.
The sample mean was the center
of mass of the observed data.
The population variance
is the expected square of
distance of a random variable from
the population around the population mean.
The sample variance is the average
square of distance of the da,
observed observations
minus the sample mean.
So we do divide by n minus 1 here
in the denominator rather than n,
and I'll talk about why in a minute.
But I also want to talk about
a conceptually, for me,
kind of difficult point, which is to talk
about the variance of the sample variance.
So let me remind you that the sample
variance is exactly a function of data.
So it is also a random variable, thus
it also has a population distribution.
That distribution has a expected value and
that expected value is
the population variance that the sample
variance is trying to estimate.
And as you collect more and
more data, the distribution of the sample
variance is going to get more concentrate,
concentrated around the population
variance it's trying to estimate.
And then I also simply want to
remind you that the square root of
the sample variance is the sample

